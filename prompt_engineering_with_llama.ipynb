{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2e6553-2d15-4070-82d0-651f595dff37",
   "metadata": {},
   "source": [
    "# Prompt Engineering with Llama 2 & 3\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dee997-4c2d-4bb3-b529-408d073598f2",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "- **Base/foundational models** - pure next word predictors, useful for developers who want to continue training for a particular task.\n",
    "\n",
    "- **Chat models** - ideal for chatbots and question/answering tasks, further trained on instructions.\n",
    "\n",
    "- **Code models** - further trained on code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726736a-0cb1-4bf6-9cec-00009c249f47",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Llama 2 comes in 3 sizes:\n",
    "1. Llama 7B chat\n",
    "2. Llama 13B chat\n",
    "3. Llama 70B chat\n",
    "\n",
    "Larger the model is, the more capacity it has to learn from its training data. Chat models are instruction-tuned on human language instructions (e.g. summarize this, tell me a joke, etc). **More common to use base foundational models for fine-tuning.**\n",
    "\n",
    "\n",
    "**Performance:**\n",
    "\n",
    "|                           | Llama2       | Falcon 40B   | GPT 3.5      |\n",
    "|---------------------------|--------------|--------------|--------------|\n",
    "| Performance (MMLU benchmark) |    68.9          | 55.4             |  70            |\n",
    "\n",
    "\n",
    "<br>**Code Llama models:**\n",
    "\n",
    "![Optional Title](imgs/code_llama_0.png)\n",
    "\n",
    "* Created by taking Llama 2 and training it for coding tasks. Comes in 3 sizes (7B, 13B, 34B).\n",
    "\n",
    "* For each size. there is a base version and an instruction fine-tuned version.\n",
    "\n",
    "* Base code Llama models are derived from the non-chat Llama models. Can be used for autocompleting/filling code.\n",
    "\n",
    "* Instruction fine-tuned models created by training the Llama chat models. So these models exhibit more human-like behavior (e.g. \"help me write some code to build some web app\" or \"please debug the following code I just wrote\". Good at human explanations of what that code is doing.\n",
    "\n",
    "* There is also Code Llama Python, which is specialized for Python coding.\n",
    "\n",
    "**Purple Llama**\n",
    "\n",
    "Umbrella project for Generative AI Safety\n",
    "* tools\n",
    "* models\n",
    "* benchmarks\n",
    "\n",
    "Two goals: \n",
    "* Make sure LLM outputs are safe to run on computers (cybersecurity)\n",
    "* Make sure LLM models are generating content that isn't harmful/toxic to humans.\n",
    "\n",
    "1. **CyberSecEval** - set of tool/benchmarks dataset to check if code completion tools are generationg secure code that guards against viruses/cyber threats.\n",
    "2. **Llama Guard** - Takes input/output of any LLM to detect harmful/toxic content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48891417-2ca0-47bc-93c6-0dd9c4a5f743",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "* *Instruction tags*: Prompting in Llama models is unique. This helps apply the recommended formatting methods.\n",
    "\n",
    "* Foundation model - https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "\n",
    "* Chat model - https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "* Try to set `repetition_penalry=1.2` or `top_p=0.95`, or `temperature=0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba1e9f2-98de-4971-a0e5-7f577b9feaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-2-7b-hf\n",
      "Temperature: 0.7\n",
      "Prompt: [INST]What's the capital of Spain?[/INST]\n",
      "\n",
      "max_tokens: 1024\n",
      "top_p: 0.95\n"
     ]
    }
   ],
   "source": [
    "from utils import query\n",
    "\n",
    "# Using (chat model): https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "response = query(\"What's the capital of Spain?\", model_name, temperature=0.7, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc63b653-5b36-419b-8957-d5d540da1227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, it isn\\'t. Madrid is not in Latin America either. It belongs to Europe and there are many differences between these two continents.\\nThen what are you complaining about if I say that Spanish-speaking countries use Spanish as their official language and English speaking countries do so too with English? You don\\'t have a point. You sound like someone who says \"I hate Americans\" while living on US soil!\\nBtw, my last post was made by me'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd34387-e954-46a9-906e-88277edfedf2",
   "metadata": {},
   "source": [
    "3 ways to access Llama:\n",
    "\n",
    "1. Hosted API service (e.g. Hugging Face, Bedrock, GCP, Azure, Together.ai, etc)\n",
    "2. Self-configured Cloud\n",
    "3. Host on your own computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b4495-f920-48f2-a1e1-58e0211952ae",
   "metadata": {},
   "source": [
    "**Prompting Llama models**\n",
    "\n",
    "![Optional Title](imgs/llama_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80c701-6b58-4cc3-a16f-8056eaf55a3d",
   "metadata": {},
   "source": [
    "For most use-cases, use the Llama chat models instead of the base foundation models. \n",
    "\n",
    "The foundational models do not understand the *instruction tags*.\n",
    "\n",
    "If you ask the foundational model \"What is the capital of France?\", it will return something like \"What is the capital of Spain?\", etc.\n",
    "\n",
    "\n",
    "Foundational models **learn to predict the next word given the words that came before it.**.\n",
    "\n",
    "The chat model might return a more human-like response to the question. The chat model was trained using instruction tags with human-like responses to queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28e682-14df-4860-86dc-25e82e2c310b",
   "metadata": {},
   "source": [
    "If you want consistent responses given the same input prompt, set `temperature` to 0. This will have the Llama model behave deterministically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0230ab1d-88db-4a3b-a98d-3f51af9f70d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-2-7b-hf\n",
      "Temperature: 0.4\n",
      "Prompt: [INST]What is the capital of France?[/INST]\n",
      "\n",
      "max_tokens: 1024\n",
      "top_p: 0.95\n"
     ]
    }
   ],
   "source": [
    "# test temperature\n",
    "response = query(\"What is the capital of France?\", model_name, temperature=0.4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0473446b-2f0e-4959-92f0-16a4b7d3f6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris\\nParis is the capital of France.\\nParis is the capital of France.Paris is the capital of France.\\n[INST]What is the capital of France?[/INST'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2066904-5768-44cd-b739-a6c803b6f4aa",
   "metadata": {},
   "source": [
    "Now let's look at `max_tokens`, let's set to 1024 by default in the helper function. On average, 1 token is about 3/4-th of a word. \n",
    "\n",
    "\n",
    "But important to note that setting a smaller number of tokens doesn't make the model give it's more complete answer more succinctly. It just stops part way through its answer.\n",
    "\n",
    "LLMs have a limit to how many tokens they can take as input, and give as output.\n",
    "\n",
    "Note that when you set `max_tokens`, this means that `input` tokens + `max_new_tokens` (output response tokens) must be less than or equal to (<=) 4097, which is the *context size* of the Llama 2 7B Chat model. Make sure that the input + output doesn't exceed this. Llama will not do this by itself, and might return an error (validation error). This means if your input is 3974 tokens, set `max_tokens` for the Llama model to 123 to not pass the 4097 token limit of Llama 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9564bb-c8c7-4d91-b50d-e4e63626f568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-2-7b-hf\n",
      "Temperature: 0.4\n",
      "Prompt: [INST]What is the capital of France?[/INST]\n",
      "\n",
      "[{'generated_text': '[INST]What is the capital of France?[/INST]\\nAns: Paris.'}]\n"
     ]
    }
   ],
   "source": [
    "# test temperature\n",
    "response = query(\"What is the capital of France?\", model_name, max_tokens=5, temperature=0.4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddafe805-58cd-4961-b925-fc43c1506eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ans: Paris.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257a796-5497-45a3-9986-427bb4f94d59",
   "metadata": {},
   "source": [
    "**Summarizing a book**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9de5d68-5176-408c-9a18-7e9cdc7f8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test_book.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4df2aac4-23c0-48f1-b04b-f97006b1a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import query\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Give me a summary of the following text in 100 words:\\n\\n\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "response = query(prompt, model_name, temperature=0.4, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b523798-1a3f-4c66-b605-5fcf92865514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### [INST]\\n\\nGive me a summary of the following text in 100 words:\\n\\n\\nTitle: The Lost World\\n\\nAuthor: Sir Arthur Conan Doyle\\n\\nIn \"The Lost World,\" Sir Arthur Conan Doyle crafts a thrilling tale of adventure and discovery in the heart of the Amazon jungle. The story follows Professor Challenger, a renowned explorer and scientist, who leads a team of'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ae14a-720b-4b04-a0ef-813a60108b2e",
   "metadata": {},
   "source": [
    "## Multi-turn Conversations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2e8075-48e3-4004-a99f-b5220907b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import query\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Give me 5 fun activities I can do this weekend.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model_name=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "response = query(prompt, model_name, temperature=0.7, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11adbf19-4662-4b31-8c03-8259e9f37230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Of course! Here are 5 fun activities you can do this weekend:\\n\\n1. Outdoor Adventure: Plan a hike, camping trip, or a picnic in a nearby park or nature reserve. Being in nature can be incredibly rejuvenating and relaxing.\\n2. Game Night: Invite some friends over for a game night. You can play board games, card games, or even video games. It's a great way to social\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2969d6-17fd-4606-913e-eeb3aab952fb",
   "metadata": {},
   "source": [
    "**Constructing multi-turn prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec5160e-14c1-4440-aa03-2ed38049e8a3",
   "metadata": {},
   "source": [
    "General form of multi-turn chat prompt:\n",
    "\n",
    "```python\n",
    "prompt_chat = f\"\"\"\n",
    "    User: {prompt 1}\n",
    "    Assistant: {response 1}\n",
    "    User: {prompt 2}\n",
    "    Assistant: {response 2}\n",
    "    User: {prompt 3}\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe75e06-a6e8-4130-b308-155cc46b5fec",
   "metadata": {},
   "source": [
    "Each turn you add in a new prompt-response pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd26fa-d96c-4b8e-bf03-924a0af29758",
   "metadata": {},
   "source": [
    "Llama-2 form of multi-turn chat prompt:\n",
    "\n",
    "\n",
    "```python\n",
    "prompt_chat = f\"\"\"\n",
    "    <s>[INST]{user prompt 1}[/INST]\n",
    "    Assistant: {model response 1}</s>\n",
    "    <s>[INST]{user prompt 2}[/INST]\n",
    "    Assistant: {model response 2}</s>\n",
    "    ...\n",
    "    <s>[INST]{user prompt 3}[/INST]\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e01551-b511-4078-9b4c-0b892f345997",
   "metadata": {},
   "source": [
    "Chat prompt ends with latest input from the user. \n",
    "\n",
    "You wrap each prompt-response pair with a set of start tags `<s>` and end tags `</s>`.\n",
    "\n",
    "You open the last prompt with a start tag `<s>`, but this time you don't close with an end tag. The turn is not over, you want the model to respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "538ed9b7-dd6d-4cca-b3bd-417e5fb0f308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-2-7b-chat-hf\n",
      "Temperature: 0.8\n",
      "Prompt: [INST]\n",
      "    What are some fun activities I can doo this weekend??\n",
      "[/INST]\n",
      "\n",
      "max_tokens: 3000\n",
      "top_p: 0.95\n",
      "There are so many fun activities you can do this weekend, depending on your interests and preferences! Here are some ideas to get you started:\n",
      "\n",
      "1. Outdoor Adventures: Go hiking, camping, or kayaking in a nearby park or nature reserve. Rent a bike and explore a new trail or explore the local beaches and go swimming.\n",
      "2. Cultural Events: Attend a concert, play, or festival in your area.\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = \"\"\"\n",
    "    What are some fun activities I can doo this weekend??\n",
    "\"\"\"\n",
    "response_1 = query(prompt_1, max_tokens=3000, verbose=True)\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eef576f-9b73-4a41-9092-aad3e2a91fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are so many fun activities you can do this weekend, depending on your interests and preferences! Here are some ideas to get you started:\n",
      "\n",
      "1. Outdoor Adventures: Go hiking, camping, or kayaking in a nearby park or nature reserve. Rent a bike and explore a new trail or explore the local beaches and go swimming.\n",
      "2. Cultural Events: Attend a concert, play, or festival in your area.\n"
     ]
    }
   ],
   "source": [
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79fb65e6-c6a9-4281-8f77-0bcd7293a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Which of these would be good for my health?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d23eaa31-268b-4aef-aa62-6f4b042a393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] \n",
      "    What are some fun activities I can doo this weekend??\n",
      " [/INST]\n",
      "There are so many fun activities you can do this weekend, depending on your interests and preferences! Here are some ideas to get you started:\n",
      "\n",
      "1. Outdoor Adventures: Go hiking, camping, or kayaking in a nearby park or nature reserve. Rent a bike and explore a new trail or explore the local beaches and go swimming.\n",
      "2. Cultural Events: Attend a concert, play, or festival in your area.\n",
      "</s>\n",
      "<s>[INST] \n",
      "Which of these would be good for my health?\n",
      " [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = f\"\"\"\n",
    "<s>[INST] {prompt_1} [/INST]\n",
    "{response_1}\n",
    "</s>\n",
    "<s>[INST] {prompt_2} [/INST]\n",
    "\"\"\"\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ccc4c74-2181-4abb-bef1-b492bb2b8c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-2-7b-chat-hf\n",
      "Temperature: 0.8\n",
      "Prompt: \n",
      "<s>[INST] \n",
      "    What are some fun activities I can doo this weekend??\n",
      " [/INST]\n",
      "There are so many fun activities you can do this weekend, depending on your interests and preferences! Here are some ideas to get you started:\n",
      "\n",
      "1. Outdoor Adventures: Go hiking, camping, or kayaking in a nearby park or nature reserve. Rent a bike and explore a new trail or explore the local beaches and go swimming.\n",
      "2. Cultural Events: Attend a concert, play, or festival in your area.\n",
      "</s>\n",
      "<s>[INST] \n",
      "Which of these would be good for my health?\n",
      " [/INST]\n",
      "\n",
      "\n",
      "max_tokens: 1024\n",
      "top_p: 0.95\n"
     ]
    }
   ],
   "source": [
    "# setting false here b/c helper function already puts\n",
    "# the instruction token for single-turn chat\n",
    "# here we are constructing part for multi-turn chat\n",
    "# so we put the instruction tags ourselves\n",
    "response_2 = query(chat_prompt,\n",
    "                 add_inst=False,\n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e97de6b0-4691-4cff-8efa-b17af832026b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of the activities I mentioned can be good for your health, depending on your current level of physical activity and any health considerations you may have. Here are some specific health benefits of each activity:\n",
      "\n",
      "1. Hiking: Hiking is a great way to get cardiovascular exercise, build muscle strength and endurance, and improve flexibility. It can also help reduce stress and improve mental health.\n",
      "2. Camping: Camping can provide opportunities\n"
     ]
    }
   ],
   "source": [
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7ac4c-fbff-47af-b59f-cc89fac1bb42",
   "metadata": {},
   "source": [
    "Number of prompts will always be greater than number of responses because we are passing prompt-response pairs, which we send to the model expecting our response back. Let's try using the `chat` utils function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab4fe6d-e5e8-4a10-b49c-221a0606b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import query, chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1957ddaf-f64b-460d-918e-fca0fe870773",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "    What are some fun activities I can doo this weekend??\n",
    "\"\"\"\n",
    "\n",
    "response_1 = query(prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf362848-45e6-4e2d-aa8f-fa144e0a00cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Which of these would be good for my health?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7bd6c97-636a-4c90-ae8f-4e133c5512d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [prompt_1, prompt_2]\n",
    "responses = [response_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a35c628-8e17-4f2d-a123-319a593d9327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Prompt:\n",
      "<s>[INST] \n",
      "    What are some fun activities I can doo this weekend??\n",
      " [/INST]\n",
      "There are plenty of fun activities you can do this weekend! Here are some ideas:\n",
      "\n",
      "1. Outdoor Adventures: Go for a hike, have a picnic in a nearby park, or go camping in the mountains.\n",
      "2. Indoor Games: Host a game night with friends, play board games or card games, or try escape room challenges.\n",
      "3. Movie Night: Rent a classic movie or a new release and have a movie marathon\n",
      "</s>\n",
      "<s>[INST] \n",
      "Which of these would be good for my health?\n",
      " [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_2 = chat(prompts, responses, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92b52625-ef19-4f14-8823-e25be837eb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All of the activities I mentioned can be good for your health, depending on your individual needs and preferences. Here are some specific benefits of each activity:\\n\\n1. Outdoor Adventures: Spending time in nature has been shown to have numerous health benefits, including reducing stress levels, improving mood, and boosting the immune system. Being physically active outdoors can also improve cardiovascular health and overall fitness.\\n2. Indoor Games'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e55b53-c14c-44cc-aae8-ab125659ae12",
   "metadata": {},
   "source": [
    "Let's keep going..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40ed2e4c-7e8e-4425-9943-bb8f4e3b8377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Prompt:\n",
      "<s>[INST] \n",
      "    What are some fun activities I can doo this weekend??\n",
      " [/INST]\n",
      "There are plenty of fun activities you can do this weekend! Here are some ideas:\n",
      "\n",
      "1. Outdoor Adventures: Go for a hike, have a picnic in a nearby park, or go camping in the mountains.\n",
      "2. Indoor Games: Host a game night with friends, play board games or card games, or try escape room challenges.\n",
      "3. Movie Night: Rent a classic movie or a new release and have a movie marathon\n",
      "</s>\n",
      "<s>[INST] \n",
      "Which of these would be good for my health?\n",
      " [/INST]\n",
      "All of the activities I mentioned can be good for your health, depending on your individual needs and preferences. Here are some specific benefits of each activity:\n",
      "\n",
      "1. Outdoor Adventures: Spending time in nature has been shown to have numerous health benefits, including reducing stress levels, improving mood, and boosting the immune system. Being physically active outdoors can also improve cardiovascular health and overall fitness.\n",
      "2. Indoor Games\n",
      "</s>\n",
      "<s>[INST] Which of these activites would be fun with friends? [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_3 = \"Which of these activites would be fun with friends?\"\n",
    "\n",
    "prompts = [prompt_1, prompt_2, prompt_3]\n",
    "responses = [response_1, response_2]\n",
    "\n",
    "response_3 = chat(prompts=prompts, responses=responses, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f96a16-0ffd-49e6-8e42-04a39831979b",
   "metadata": {},
   "source": [
    "## Prompt Engineering Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c4cee-9e7b-4951-837e-23761b91f822",
   "metadata": {},
   "source": [
    "**Prompt Engineering**: The science and art of communicating with an LLM, so that it responds and behaves in a way that's useful for you.\n",
    "\n",
    "You can guide the model to improve its response for your task through specific instructions or by including different kinds of information, or \"context\", e.g.\n",
    "* Providing **examples of the task** you are trying to carry out\n",
    "* Specifying how to **format responses**\n",
    "* Requesting that the model assume a particular **\"role or persona\"** when creating its response\n",
    "* Including **additional information or data** for the model to use in its response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab85aa-33a9-4b9f-8a4d-3924136336df",
   "metadata": {},
   "source": [
    "### Zero-shot Prompting\n",
    "\n",
    "- Here is an example of zero-shot prompting.\n",
    "- You are prompting the model to see if it can infer the task from the structure of your prompt.\n",
    "- In zero-shot prompting, you only provide the structure to the model, but without any examples of the completed task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d0121bf-8244-43fe-98f8-be766d92694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment in the message \"Hi Amit, thanks for the thoughtful birthday card!\" is \"Gratitude\".\n"
     ]
    }
   ],
   "source": [
    "from utils import query, chat\n",
    "\n",
    "prompt = \"\"\"\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaad847-815a-4996-9714-f89ad7a1f90b",
   "metadata": {},
   "source": [
    "**In-context learning**: LLMs can determine the task you want them to perform from examples in your prompt! This is called **zero-shot prompt**. Some LLMs cannot do this. Some models may respond with its base behavior, and just continue generating text. Can build upon this by providing one or more examples of what we're trying to do, this can help the model **infer the task**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848fb69-f0a2-48a6-bac5-1fac00edcc8c",
   "metadata": {},
   "source": [
    "### Few-shot Prompting\n",
    "\n",
    "- Here is an example of few-shot prompting.\n",
    "- In few-shot prompting, you not only provide the structure to the model, but also two or more examples.\n",
    "- You are prompting the model to see if it can infer the task from the structure, as well as the examples in your prompt.\n",
    "- Prompting with 1 example is called **one-shot prompting**.\n",
    "- Prompting with more than 1 is called **few-shot prompting** or **n-shot prompting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b162cb0-7137-4b60-a7ec-d3636e3125a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are the sentiments for each message:\n",
      "\n",
      "1. Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "2. Message: Can't wait to order pizza for dinner tonight\n",
      "Sentiment: Positive\n",
      "3. Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = query(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb94a59-53a8-4908-b5b5-7f46e88c8ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here are the sentiments for each message:\\n\\n1. Message: Hi Dad, you're 20 minutes late to my piano recital!\\nSentiment: Negative\\n2. Message: Can't wait to order pizza for dinner tonight\\nSentiment: Positive\\n3. Message: Hi Amit, thanks for the thoughtful birthday card!\\nSentiment: Positive\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c37a57a-a9cf-4dcb-b5b6-8132117776f5",
   "metadata": {},
   "source": [
    "### Specifying the Output Format\n",
    "\n",
    "- You can also specify the format in which you want the model to respond.\n",
    "- In the example below, you are asking to \"give a one word response\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78bf1a-91e3-4dbc-ae74-81b7f5e40e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba9673-8d81-443e-9ef2-9f2ec0ce5d2d",
   "metadata": {},
   "source": [
    "**Note:** For all the examples above, you used the 7 billion parameter model, `llama-2-7b-chat`. And as you saw in the last example, the 7B model was uncertain about the sentiment.\n",
    "\n",
    "- You can use the larger (70 billion parameter) `llama-2-70b-chat` model to see if you get a better, certain response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab04f9-9642-46c8-ab19-3a0df45b0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                model=\"togethercomputer/llama-2-70b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523689b-8a0f-49fe-a964-1967b74ee2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Now, use the smaller model again, but adjust your prompt in order to help the model to understand what is being expected from it.\n",
    "- Restrict the model's output format to choose from `positive`, `negative` or `neutral`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758c41d-517f-413a-ab1c-8804533703c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: \n",
    "\n",
    "Respond with either positive, negative, or neutral.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205660be-1326-48b8-a620-47a03b6369e7",
   "metadata": {},
   "source": [
    "### Role Prompting\n",
    "\n",
    "- Roles give context to LLMs what type of answers are desired.\n",
    "- Llama 2 often gives more consistent responses when provided with a role.\n",
    "- First, try standard prompt and see the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999fff8d-d5ea-43a1-a899-174af25d7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe6d26-429e-4f4a-b489-526097b61c0f",
   "metadata": {},
   "source": [
    "- Now, try it by giving the model a \"role\", and within the role, a \"tone\" using which it should respond with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799fd024-941f-4f3c-9ca6-98ffc234cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"\"\"\n",
    "Your role is a life coach \\\n",
    "who gives advice to people about living a good life.\\\n",
    "You attempt to provide unbiased advice.\n",
    "You respond in the tone of an English pirate.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{role}\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b93aba7-580a-47b9-b85c-6a8cf4a3b69a",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "- Summarizing a large text is another common use case for LLMs. Let's try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c040b83-9107-446a-b42d-c3f3a294f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear Amit,\n",
    "\n",
    "An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n",
    "\n",
    "Here are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n",
    "\n",
    "Prompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\n",
    "One-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task — the input and the desired output — sometimes yields better results.\n",
    "Fine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\n",
    "Pretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\n",
    "For most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If you’re unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesn’t work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesn’t deliver the performance you want, then try fine-tuning — but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n",
    "\n",
    "(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? 😜)\n",
    "\n",
    "Additional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, that’s not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLM’s output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning — in which GPT-4 surpasses current open models — it can be difficult to fine-tune a smaller model to deliver superior results.\n",
    "\n",
    "Beyond choosing a development approach, it’s also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. I’ll talk about how to make this choice in a future letter.\n",
    "\n",
    "Keep learning!\n",
    "\n",
    "Andrew\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b53c7-4654-4793-a898-c2b92ace1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize this email and extract some key points.\n",
    "What did the author say about llama models?:\n",
    "\n",
    "email: {email}\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e96fe-57fb-431e-8229-6ca2afc4001a",
   "metadata": {},
   "source": [
    "### Providing New Information in the Prompt\n",
    "- A model's knowledge of the world ends at the moment of its training - so it won't know about more recent events.\n",
    "- Llama 2 was released for research and commercial use on July 18, 2023, and its training ended some time before that date.\n",
    "- Ask the model about an event, in this case, FIFA Women's World Cup 2023, which started on July 20, 2023, and see how the model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb18808-093f-4bba-9927-49ddac71d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Who won the 2023 Women's World Cup?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbf11b-593c-4b5c-b4d1-9e3d530f327d",
   "metadata": {},
   "source": [
    "- As you can see, the model still thinks that the tournament is yet to be played, even though you are now in 2024!\n",
    "- Another thing to **note** is, July 18, 2023 was the date the model was released to public, and it was trained even before that, so it only has information upto that point. The response says, \"the final match is scheduled to take place in July 2023\", but the final match was played on August 20, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068b918-75c4-4112-b122-53df9dd068cc",
   "metadata": {},
   "source": [
    "- You can provide the model with information about recent events, in this case text from Wikipedia about the 2023 Women's World Cup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31bc6a-f751-4026-a067-a0e5108413fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "The 2023 FIFA Women's World Cup (Māori: Ipu Wahine o te Ao FIFA i 2023)[1] was the ninth edition of the FIFA Women's World Cup, the quadrennial international women's football championship contested by women's national teams and organised by FIFA. The tournament, which took place from 20 July to 20 August 2023, was jointly hosted by Australia and New Zealand.[2][3][4] It was the first FIFA Women's World Cup with more than one host nation, as well as the first World Cup to be held across multiple confederations, as Australia is in the Asian confederation, while New Zealand is in the Oceanian confederation. It was also the first Women's World Cup to be held in the Southern Hemisphere.[5]\n",
    "This tournament was the first to feature an expanded format of 32 teams from the previous 24, replicating the format used for the men's World Cup from 1998 to 2022.[2] The opening match was won by co-host New Zealand, beating Norway at Eden Park in Auckland on 20 July 2023 and achieving their first Women's World Cup victory.[6]\n",
    "Spain were crowned champions after defeating reigning European champions England 1–0 in the final. It was the first time a European nation had won the Women's World Cup since 2007 and Spain's first title, although their victory was marred by the Rubiales affair.[7][8][9] Spain became the second nation to win both the women's and men's World Cup since Germany in the 2003 edition.[10] In addition, they became the first nation to concurrently hold the FIFA women's U-17, U-20, and senior World Cups.[11] Sweden would claim their fourth bronze medal at the Women's World Cup while co-host Australia achieved their best placing yet, finishing fourth.[12] Japanese player Hinata Miyazawa won the Golden Boot scoring five goals throughout the tournament. Spanish player Aitana Bonmatí was voted the tournament's best player, winning the Golden Ball, whilst Bonmatí's teammate Salma Paralluelo was awarded the Young Player Award. England goalkeeper Mary Earps won the Golden Glove, awarded to the best-performing goalkeeper of the tournament.\n",
    "Of the eight teams making their first appearance, Morocco were the only one to advance to the round of 16 (where they lost to France; coincidentally, the result of this fixture was similar to the men's World Cup in Qatar, where France defeated Morocco in the semi-final). The United States were the two-time defending champions,[13] but were eliminated in the round of 16 by Sweden, the first time the team had not made the semi-finals at the tournament, and the first time the defending champions failed to progress to the quarter-finals.[14]\n",
    "Australia's team, nicknamed the Matildas, performed better than expected, and the event saw many Australians unite to support them.[15][16][17] The Matildas, who beat France to make the semi-finals for the first time, saw record numbers of fans watching their games, their 3–1 loss to England becoming the most watched television broadcast in Australian history, with an average viewership of 7.13 million and a peak viewership of 11.15 million viewers.[18]\n",
    "It was the most attended edition of the competition ever held.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b13807-933b-4e8b-96f4-4be9268ba666",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the following context, who won the 2023 Women's World cup?\n",
    "context: {context}\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2b0f0-9f66-4af1-b531-228485a17516",
   "metadata": {},
   "source": [
    "### Try it Yourself!\n",
    "\n",
    "Try asking questions of your own! Modify the code below and include your own context to see how the model responds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea95be-f6d9-4305-ba2c-82d85dc38d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "<paste context in here>\n",
    "\"\"\"\n",
    "query = \"<your query here>\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given the following context,\n",
    "{query}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                 verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba14950b-e397-4b1e-a832-e4bc8a630850",
   "metadata": {},
   "source": [
    "### Chain-of-thought Prompting\n",
    "- LLMs can perform better at reasoning and logic problems if you ask them to break the problem down into smaller steps. This is known as **chain-of-thought** prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743d709-b7f1-45b3-9e6d-565498f71bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e8b734-91d7-4b47-a7ee-62cb68d1a43d",
   "metadata": {},
   "source": [
    "- Modify the prompt to ask the model to \"think step by step\" about the math problem you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6863767-7433-4cea-bc3f-ff516604959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72311d-3dbc-465a-8966-5a17f2acc620",
   "metadata": {},
   "source": [
    "- Provide the model with additional instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ca255-de01-4786-8d08-3c6687b47562",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "Explain each intermediate step.\n",
    "Only when you are done with all your steps,\n",
    "provide the answer based on your intermediate steps.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65992ce-f540-4e24-b801-9270505dac3f",
   "metadata": {},
   "source": [
    "- The order of instructions matters!\n",
    "- Ask the model to \"answer first\" and \"explain later\" to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf2515-cfd6-4e6f-a160-b4c6c4a6acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "Think step by step.\n",
    "Provide the answer as a single yes/no answer first.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123b12b5-25e0-433f-92fd-ae8f91bc1ead",
   "metadata": {},
   "source": [
    "- Since LLMs predict their answer one token at a time, the best practice is to ask them to think step by step, and then only provide the answer after they have explained their reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755eb07-961a-4417-b97f-999c040e5010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
